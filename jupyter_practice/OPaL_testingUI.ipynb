{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to scrape websites and return json objects.\n",
    "\n",
    "Called OPaL: Opposing Positions and Lingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'url_catcher'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murl_catcher\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_all_news_urls, main\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'url_catcher'"
     ]
    }
   ],
   "source": [
    "from url_catcher import get_all_news_urls, main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(urls:list):\n",
    "    responses = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            responses.append(response.text)\n",
    "        except requests.exceptions.RequestException as exception:\n",
    "            if hasattr(exception, 'response') and exception.response is not None:\n",
    "                status_code = exception.response.status_code\n",
    "                raise requests.HTTPError(f\"HTTP Error: {status_code} occured\", \n",
    "                                    response=exception.response)\n",
    "            raise requests.HTTPError(\"Non-Specific Request Error: 500\")\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1819 specific parser\n",
    "\n",
    "def article_parser_1819(urls):\n",
    "    #create request object by calling the make_request function\n",
    "    \n",
    "    request_responses = make_request(urls)\n",
    "    all_articles = []\n",
    "\n",
    "    #parse the request_responses\n",
    "    for request_response in request_responses:\n",
    "        soup = BeautifulSoup(request_response, 'html.parser')\n",
    "        json_soup = {\n",
    "            'title': '',\n",
    "            'author': '',\n",
    "            'date': '',\n",
    "            'line_count': 0,\n",
    "            'line_content': {}\n",
    "            }\n",
    "\n",
    "        #pull the title element and add it to the json object\n",
    "        title = soup.title.string\n",
    "        title = title.strip()\n",
    "        json_soup['title'] = title\n",
    "\n",
    "        #extract author and date\n",
    "        author_date_div = soup.find('div', class_='author-date')\n",
    "        author_name = author_date_div.find('a').text\n",
    "        date = author_date_div.text.split('|')[1].strip()\n",
    "\n",
    "        #assign author and date to json object\n",
    "        json_soup['author'] = author_name\n",
    "        json_soup['date'] = date\n",
    "\n",
    "        #pull the paragraph elements and create an empty array for the contents\n",
    "        paragraphs = soup.find_all(['p'])\n",
    "        paragraph_texts = []\n",
    "\n",
    "\n",
    "        for p in paragraphs:\n",
    "            # Get the text and strip whitespace\n",
    "            text = p.get_text().strip()\n",
    "            # Split by line breaks that might be in the HTML\n",
    "            lines = text.split('\\n')\n",
    "            # Add each non-empty line\n",
    "            for line in lines:\n",
    "                if line.strip():  # Only add non-empty lines\n",
    "                    paragraph_texts.append(line.strip())\n",
    "        \n",
    "        # create a json object of paragraph lines and line contents\n",
    "        json_soup['line_count'] = len(paragraph_texts)\n",
    "        for i, line in enumerate(paragraph_texts, 1):\n",
    "            json_soup['line_content'][f\"line {i}\"] = line\n",
    "        \n",
    "        all_articles.append(json_soup)\n",
    "\n",
    "    return json.dumps(all_articles, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_all_news_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m max_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Adjust this number based on how many pages you want to scrape\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the URLs using the url_catcher function\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m urls \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_news_urls\u001b[49m(base_url, suffix, max_pages)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Process those URLs with your article parser\u001b[39;00m\n\u001b[1;32m     12\u001b[0m result \u001b[38;5;241m=\u001b[39m article_parser_1819(urls)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_all_news_urls' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Get URLs from url_catcher\n",
    "    base_url = 'https://1819news.com/'\n",
    "    suffix = '/news/item'\n",
    "    max_pages = 3  # Adjust this number based on how many pages you want to scrape\n",
    "    \n",
    "    # Get the URLs using the url_catcher function\n",
    "    urls = get_all_news_urls(base_url, suffix, max_pages)\n",
    "    \n",
    "    # Process those URLs with your article parser\n",
    "    result = article_parser_1819(urls)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
